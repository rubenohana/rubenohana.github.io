<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<style>
div {
  text-align: justify;
  text-justify: inter-word;
}
</style>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Ruben Ohana</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">

<div class="menu-item">Useful links:</div>
<div class="menu-item"><a href="CV_Public.pdf" target="_blank">CV</a></div>
<div class="menu-item"><a href="https://scholar.google.fr/citations?user=F9qNg2wAAAAJ&hl=en" target="_blank"> Google Scholar</a></div>
<!--<div class="menu-item"><a href="https://www.semanticscholar.org/author/Ruben-Ohana/1379502843" target="_blank"> Semantic Scholar</a></div>-->
<div class="menu-item"><a href="https://twitter.com/oharub" target="_blank"> Twitter</a></div>
<div class="menu-item"><a href="https://www.linkedin.com/in/rubenohana/" target="_blank"> LinkedIn</a></div>

</td>
<td id="layout-content">
<div id="toptitle">
<h1>Ruben Ohana</h1>
</div>
<table class="imgtable"><tr><td>
<img src="photo2.jpg" alt="" width="170px" />&nbsp;</td>
<td align="left"><h2>Briefly</h2>
    <p> <b>Personal news:</b> in October 2022, I will start the position of Flatiron Research Fellow at the <a href = "https://www.simonsfoundation.org/flatiron/center-for-computational-mathematics/"> Center for Computational Mathematics of the Flatiron Institute </a> (Simons Foundation) in New York City! </p>
<p> <div><b>I am a third-year Ph.D. student</b> under the supervision of <a href="https://florentkrzakala.com/"> Florent Krzakala</a> (EPFL, formerly at ENS),<a href="https://www.di.ens.fr/~rudi/"> Alessandro Rudi</a> (INRIA - DIENS, Sierra Team) and  <a href="https://scholar.google.fr/citations?user=PCIAcfUAAAAJ&hl=en">Laurent Daudet</a> (LightOn). I do research in Machine Learning at the <a href="https://www.ens.psl.eu "> Ecole Normale Supérieure</a> in Paris, in collaboration with the startup <a href="https://lighton.ai/" > LightOn</a>.
<p>My research interests focus on random features for kernel approximation, high-dimensional probabilities, random matrices and alternative training methods for deep learning. More precisely, my current work focuses on using Random Features to approximate kernel methods and to study deep learning frameworks such as Reservoir Computing. I am also looking for new machine learning applications of the <a href="https://lighton.ai/our-technology/" > Optical Processing Unit </a> developed by LightOn in kernel approximations, adversarial robustness or differential privacy.
<p>I also had the chance to do a 4 months PhD internship at the <a href ="https://ailab.criteo.com/">Criteo AI Lab</a> under the supervision of <a href="https://pageperso.lis-lab.fr/~liva.ralaivola/doku.php"> Liva Ralaivola</a> and <a href = "https://scholar.google.fr/citations?user=INHspc0AAAAJ&hl=en"> Alain Rakotomamonjy</a> where we worked on building a PAC-Bayesian framework for Sliced-Wasserstein distances.</p>
<p><b>Before starting my Ph.D.,</b> I graduated with an engineering degree in Physics from <a href="https://www.espci.psl.eu/en/"> ESPCI Paris</a>, an MSc in Condensed Matter from the  <a href="https://www.phys.ens.fr/spip.php?rubrique284&lang=en"> Master ICFP</a> at the <a href="https://www.ens.psl.eu "> Ecole Normale Supérieure</a> and an MSc in Statistics/Machine Learning from the Master in Mathematics at <a href="https://www.sorbonne-universite.fr/en/university"> Sorbonne University</a>.
I did an internship at <a href="http://www.brl.ntt.co.jp/E/">NTT Basic Research Laboratory</a> in Japan, where I worked on the Quantum spin Hall Effect in InAS/GaSb double quantum wells, experimentally and theoretically under the supervision of <a href="http://www.brl.ntt.co.jp/people/irie.hiroshi/">Irie Hiroshi</a>. I also did my first master thesis at the <a href="https://www.ligo.caltech.edu/"> LIGO</a> (the gravitational wave observatory), at the <a href="https://www.mit.edu/">Massachussetts Institute of Technology</a>, where I studied the generation of lasers that will be integrated in the next update of the interferometer, under the supervision of <a href="https://space.mit.edu/people/fritschel-peter-k/">Peter Fritschel</a>. I did my second master thesis at the <a href="http://www.quantuminfolip6.fr/">Quantum Information Group</a> at the <a href="https://www.lip6.fr/?LANG=en LIP6"> LIP6</a> at Sorbonne University where I studied Contextuality in quantum information networks, under the supervision of <a href="https://damianmarkham.weebly.com/">Damian Markham</a>.  	</div>


<h2>Contact</h2>
<ul>
<li><p>E-mail: ruben [dot] ohana [at] phys [dot] ens [dot] fr <br />
</li>
<li><p>Physical address: LPENS, Ecole Normale Supérieure <a href="https://www.google.com/maps/place/24+Rue+Lhomond,+75005+Paris/data=!4m2!3m1!1s0x47e671e905a58849:0x6c1d0dc24c54a979?sa=X&ved=2ahUKEwiVqMvA86HrAhULzoUKHZCRAA0Q8gEwAHoECAsQAQ">   24 rue Lhomond, 75005 Paris </a> </p> 
</li>
</ul>
</td></tr></table>
<h2>Publications and Preprints</h2>

<ul>
<li><p><i>Shedding a PAC-Bayesian Light on Adaptive Sliced-Wasserstein Distances. </i> <b>R. Ohana<sup>*</sup></b>, K. Nadjahi<sup>*</sup>, A. Rakotomamonjy, L. Ralaivola.  2022, preprint. [<a href="https://arxiv.org/abs/2206.03230">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> The Sliced-Wasserstein distance (SW) is a computationally efficient and theoretically grounded alternative to the Wasserstein distance. Yet, the literature on its statistical properties with respect to the distribution of slices, beyond the uniform measure, is scarce. To bring new contributions to this line of research, we leverage the PAC-Bayesian theory and the central observation that SW actually hinges on a slice-distribution-dependent Gibbs risk, the kind of quantity PAC-Bayesian bounds have been designed to characterize. We provide four types of results: i) PAC-Bayesian generalization bounds that hold on what we refer as adaptive Sliced-Wasserstein distances, i.e. distances defined with respect to any distribution of slices, ii) a procedure to learn the distribution of slices that yields a maximally discriminative SW, by optimizing our PAC-Bayesian bounds, iii) an insight on how the performance of the so-called distributional Sliced-Wasserstein distance may be explained through our theory, and iv) empirical illustrations of our findings. </div></p>
</li>
</ul>

<ul>
<li><p><i>Complex-to-Real Random Features for Polynomial Kernels. </i> J. Wacker, <b>R. Ohana</b>, M. Filippone.  2022, preprint. [<a href="https://arxiv.org/abs/2202.02031">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> Kernel methods are ubiquitous in statistical modeling due to their theoretical guarantees as well as their competitive empirical performance. Polynomial kernels are of particular importance as their feature maps model the interactions between the dimensions of the input data. However, the construction time of explicit feature maps scales exponentially with the polynomial degree and a naive application of the kernel trick does not scale to large datasets. In this work, we propose Complex-to-Real (CtR) random features for polynomial kernels that leverage intermediate complex random projections and can yield kernel estimates with much lower variances than their real-valued analogs. The resulting features are real-valued, simple to construct and have the following advantages over the state-of-the-art: 1) shorter construction times, 2) lower kernel approximation errors for commonly used degrees, 3) they enable us to obtain a closed-form expression for their variance. </li>
</ul>

<ul>
<li><p><i>Photonic Differential Privacy with Direct Feedback Alignment. </i> <b>R. Ohana<sup>*</sup></b>, H.J. Ruiz<sup>*</sup>, J. Launay<sup>*</sup>, A. Cappelli, I. Poli, L. Ralaivola, A. Rakotomamonjy.  2021, <i style="color:green;"> NeurIPS 2021.</i> [<a href="https://proceedings.neurips.cc/paper/2021/file/b8c4c8b2271787e2f78b5fe2ce193caa-Paper.pdf">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> Optical Processing Units (OPUs) -- low-power photonic chips dedicated to large scale random projections -- have been used in previous work to train deep neural networks using Direct Feedback Alignment (DFA), an effective alternative to backpropagation. Here, we demonstrate how to leverage the intrinsic noise of optical random projections to build a differentially private DFA mechanism, making OPUs a solution of choice to provide a private-by-design training. We provide a theoretical analysis of our adaptive privacy mechanism, carefully measuring how the noise of optical random projections propagates in the process and gives rise to provable Differential Privacy. Finally, we conduct experiments demonstrating the ability of our learning procedure to achieve solid end-task performance. 
</div></p>
</li>
</ul>

<ul>
<li><p><i>ROPUST: Improving Robustness through Fine-tuning with Photonic Processors and Synthetic Gradients. </i> A. Cappelli, J. Launay, L. Meunier, <b> R. Ohana,</b> I. Poli.  2021, preprint. [<a href="https://arxiv.org/abs/2108.04217">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> Robustness to adversarial attacks is typically obtained through expensive adversarial training with Projected Gradient Descent. Here we introduce ROPUST, a remarkably simple and efficient method to leverage robust pre-trained models and further increase their robustness, at no cost in natural accuracy. Our technique relies on the use of an Optical Processing Unit (OPU), a photonic co-processor, and a fine-tuning step performed with Direct Feedback Alignment, a synthetic gradient training scheme. We test our method on nine different models against four attacks in RobustBench, consistently improving over state-of-the-art performance. We perform an ablation study on the single components of our defense, showing that robustness arises from parameter obfuscation and the alternative training method. We also introduce phase retrieval attacks, specifically designed to increase the threat level of attackers against our own defense. We show that even with state-of-the-art phase retrieval techniques, ROPUST remains an effective defense.  
</div></p>
</li>
</ul>

<ul>
<li><p><i>Adversarial Robustness by Design through Analog Computing and Synthetic Gradients. </i><b>R. Ohana<sup>*</sup></b>, A. Cappelli<sup>*</sup>, J. Launay, L. Meunier, I. Poli, F. Krzakala.  2021, <i style="color:green;"> ICASSP 2022.</i> [<a href="https://arxiv.org/abs/2101.02115">arXiv</a>, <a href="https://github.com/lightonai/adversarial-robustness-by-design">Github</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> We propose a new defense mechanism against adversarial attacks inspired by an optical co-processor, providing robustness without compromising natural accuracy in both white-box and black-box settings. This hardware co-processor performs a nonlinear fixed random transformation, where the parameters are unknown and impossible to retrieve with sufficient precision for large enough dimensions. In the white-box setting, our defense works by obfuscating the parameters of the random projection. Unlike other defenses relying on obfuscated gradients, we find we are unable to build a reliable backward differentiable approximation for obfuscated parameters. Moreover, while our model reaches a good natural accuracy with a hybrid backpropagation - synthetic gradient method, the same approach is suboptimal if employed to generate adversarial examples. We find the combination of a random projection and binarization in the optical system also improves robustness against various types of black-box attacks. Finally, our hybrid training method builds robust features against transfer attacks. We demonstrate our approach on a VGG-like architecture, placing the defense on top of the convolutional features, on CIFAR-10 and CIFAR-100.
</div></p>
</li>
</ul>

<ul>
<li><p><i>Photonic co-processors in HPC: using LightOn OPUs for Randomized Numerical Linear Algebra. </i> D. Hesslow, A. Cappelli, I. Carron, L. Daudet, R. Lafargue, K. Muller, <b>R. Ohana</b>, G. Pariente, I. Poli.  2021, <i style="color:green;">Hot Chips 2021.</i> [<a href="https://arxiv.org/abs/2104.14429">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> Randomized Numerical Linear Algebra (RandNLA) is a powerful class of methods, widely used in High Performance Computing (HPC). RandNLA provides approximate solutions to linear algebra functions applied to large signals, at reduced computational costs. However, the randomization step for dimensionality reduction may itself become the computational bottleneck on traditional hardware. Leveraging near constant-time linear random projections delivered by LightOn Optical Processing Units we show that randomization can be significantly accelerated, at negligible precision loss, in a wide range of important RandNLA algorithms, such as RandSVD or trace estimators. 
</div></p>
</li>
</ul>

<ul>
<li><p><i>The dynamics of learning with feedback alignment. </i> M. Refinetti<sup>*</sup>, S. d'Ascoli<sup>*</sup>, <b> R. Ohana</b>, S. Goldt.  2021,  <i style="color:green;"> ICML 2021. </i> [<a href="https://arxiv.org/abs/2011.12428">arXiv</a>, <a href="https://github.com/sdascoli/dfa-landscape">Github</a>, <a href="https://twitter.com/sdgoldt/status/1337076670025568257"> Twitter thread</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> Direct Feedback Alignment (DFA) is emerging as an efficient and biologically plausible alternative to the ubiquitous backpropagation algorithm for training deep neural networks. Despite relying on random feedback weights for the backward pass, DFA successfully trains state-of-the-art models such as Transformers. On the other hand, it notoriously fails to train convolutional networks. An understanding of the inner workings of DFA to explain these diverging results remains elusive. Here, we propose a theory for the success of DFA. We first show that learning in shallow networks proceeds in two steps: an alignment phase, where the model adapts its weights to align the approximate gradient with the true gradient of the loss function, is followed by a memorisation phase, where the model focuses on fitting the data. This two-step process has a degeneracy breaking effect: out of all the low-loss solutions in the landscape, a network trained with DFA naturally converges to the solution which maximises gradient alignment. We also identify a key quantity underlying alignment in deep linear networks: the conditioning of the alignment matrices. The latter enables a detailed understanding of the impact of data structure on alignment, and suggests a simple explanation for the well-known failure of DFA to train convolutional neural networks. Numerical experiments on MNIST and CIFAR10 clearly demonstrate degeneracy breaking in deep non-linear networks and show that the align-then-memorize process occurs sequentially from the bottom layers of the network to the top. 
</div></p>
</li>
</ul>

<ul>
<li><p><i>Experimental Approach to Demonstrating Contextuality for Qudits. </i>A. Sohbi, <b> R. Ohana</b>, I. Zaquine, E. Diamanti, D. Markham. 2020. <i style="color:green;"> Physical Review A.</i> [<a href="https://arxiv.org/abs/2010.13278">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> We propose a method to experimentally demonstrate contextuality with a family of tests for qudits. The experiment we propose uses a qudit encoded in the path of a single photon and its temporal degrees of freedom. We consider the impact of noise on the effectiveness of these tests, taking the approach of ontologically faithful non-contextuality. In this approach, imperfections in the experimental set up must be taken into account in any faithful ontological (classical) model, which limits how much the statistics can deviate within different contexts. In this way we bound the precision of the experimental setup under which ontologically faithful non-contextual models can be refuted. We further consider the noise tolerance through different types of decoherence models on different types of encodings of qudits. We quantify the effect of the decoherence on the required precision for the experimental setup in order to demonstrate contextuality in this broader sense.  
</div></p>
</li>
</ul>

<ul>
<li><p> <i>Reservoir Computing meets Recurrent Kernels and Structured Transforms. </i> <b>R. Ohana<sup>*</sup></b>, J. Dong<sup>*</sup>, M. Rafayelyan, F. Krzakala. <i style="color:green;"><b>Oral</b> @ NeurIPS 2020.</i> [<a href="https://papers.nips.cc/paper/2020/file/c348616cd8a86ee661c7c98800678fad-Paper.pdf" >NeurIPS</a>, <a href="https://nips.cc/virtual/2020/public/session_oral_21072.html" > Oral (starts at 46:30)</a>, <a href="https://arxiv.org/abs/2006.07310">arXiv</a>, <a href="https://github.com/rubenohana/Reservoir-computing-kernels">Github</a>, <a href="https://twitter.com/oharub/status/1319634001330667521"> Twitter thread</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> Reservoir Computing is a class of simple yet efficient Recurrent Neural Networks where internal weights are fixed at random and only a linear output layer is trained. In the large size limit, such random neural networks have a deep connection with kernel methods. Our contributions are threefold: a) We rigorously establish the recurrent kernel limit of Reservoir Computing and prove its convergence. b) We test our models on chaotic time series prediction, a classic but challenging benchmark in Reservoir Computing, and show how the Recurrent Kernel is competitive and computationally efficient when the number of data points remains moderate. c) When the number of samples is too large, we leverage the success of structured Random Features for kernel approximation by introducing Structured Reservoir Computing. The two proposed methods, Recurrent Kernel and Structured Reservoir Computing, turn out to be much faster and more memory-efficient than conventional Reservoir Computing. 
</div></p>
</li>
</ul>

<ul>
<li><p> <i>Kernel Computations from large-scale random features obtained by Optical Processing Units. </i><b>R. Ohana</b>, J. Wacker, J. Dong, S. Marmin, F. Krzakala, M. Filippone, L. Daudet.  <i style="color:green;">ICASSP 2020.</i> [<a href="https://ieeexplore.ieee.org/abstract/document/9053272">ICASSP</a>,  <a href="https://arxiv.org/abs/1910.09880">arXiv</a>, <a href="https://github.com/joneswack/opu-kernel-experiments"> Github</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> Approximating kernel functions with random features (RFs) has been a successful application of random projections for nonparametric estimation. However, performing random projections presents computational challenges for large-scale problems. Recently, a new optical hardware called Optical Processing Unit (OPU) has been developed for fast and energy-efficient computation of large-scale RFs in the analog domain. More specifically, the OPU performs the multiplication of input vectors by a large random matrix with complex-valued i.i.d. Gaussian entries, followed by the application of an element-wise squared absolute value operation - this last nonlinearity being intrinsic to the sensing process. In this paper, we show that this operation results in a dot-product kernel that has connections to the polynomial kernel, and we extend this computation to arbitrary powers of the feature map. Experiments demonstrate that the OPU kernel and its RF approximation achieve competitive performance in applications using kernel ridge regression and transfer learning for image classification. Crucially, thanks to the use of the OPU, these results are obtained with time and energy savings. 
</div></p>
</li>
</ul>

<ul>
<li><p><i>Impact of epitaxial strain on the topological-nontopological phase diagram and semimetallic behavior of InAs/GaSb composite quantum wells.</i> H. Irie, T. Akiho, F. Couedo, <b>R. Ohana</b>, S. Suzuki, H. Onomitsu, K. Muraki.  2020, <i style="color:green;">Physical Review B.</i> [<a href="https://journals.aps.org/prb/abstract/10.1103/PhysRevB.101.075433">Phys. Rev. B</a>, <a href="https://arxiv.org/abs/2002.12503v1">arXiv</a>] <span class="toggle-trigger">[Show Abstract]<span>
<div class="toggle-wrap">
<b>Abstract:</b> We study the influence of epitaxial strain on the electronic properties of InAs/GaSb composite quantum wells (CQWs), host structures for quantum spin Hall insulators, by transport measurements and eight-band <i>k⋅p</i> calculations. Using different substrates and buffer layer structures for crystal growth, we prepare two types of samples with vastly different strain conditions. CQWs with a nearly strain-free GaSb layer exhibit a resistance peak at the charge neutrality point that reflects the opening of a topological gap in the band-inverted regime. In contrast, for CQWs with 0.50% biaxial tensile strain in the GaSb layer, semimetallic behavior indicating a gap closure is found for the same degree of band inversion. Additionally, with the tensile strain, the boundary between the topological and nontopological regimes is located at a larger InAs thickness. Eight-band <i>k⋅p</i>  calculations reveal that tensile strain in GaSb not only shifts the phase boundary but also significantly modifies the band structure, which can result in the closure of an indirect gap and make the system semimetallic even in the topological regime. Our results thus provide a global picture of the topological-nontopological phase diagram as a function of layer thicknesses and strain. 
</div></p>
</li>
</ul>


    <h2>Patent</h2>
    <ul>
        <li><p><i>Method and System for machine learning using optical data</i> I. Poli, J. Launay, K. Müller, G. Pariente, I. Carron, L. Daudet, <b>R. Ohana</b>, D. Hesslow. 2021, <i style="color:green;">US Patent </i> [<a href="https://patentimages.storage.googleapis.com/a1/34/7c/3c91a8598ebe1f/US20210287079A1.pdf">Patent</a>] <div class="toggle-wrap">
<b>Abstract:</b>A system may include an optical source and an adjustable spatial light modulator coupled to the optical source. The system may further include a medium coupled to the adjustable spatial light modulator, and an optical detector coupled to the medium. The optical detector may obtain various optical signals that are transmitted through the medium at various predetermined spatial light modulations using the adjustable spatial light modulator. The system may further include a controller coupled to the optical detector and the adjustable spatial light modulator. The controller may train an electronic model using various synthetic gradients based on the optical signals.</div></p>
</li>
</ul>
    <h2>Miscellaneous</h2>
    <ul><li>Reviewer for ALT 2020, NeurIPS 2021, NeurIPS 2022, ICML 2022, WACV 2022, Nature Communications, Journal of Machine Learning Research (JMLR). </li>

    <li>Oral at NeurIPS 2020, Paris Machine learning meetup 2020, Roscoff meeting 2019, invited speaker at the LAMSADE of  Dauphine, Golosino meetings at ENS 2019-2020.</li>
    </ul>

<div id="footer">
<div id="footer-text">
Page updated on the 26th of August 2022</a>.
</div>
</div>
</td>
</tr>
</table>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script>
$(document).ready(function() {
    $(".toggle-trigger").click(function() {
        $(this).parent().nextAll('.toggle-wrap').first().slideToggle('slow');
    });
});
</script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-128753599-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
